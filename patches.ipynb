{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import timm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from leaf.dta import LeafDataset, GetPatches, TransformPatches, RandomGreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms with normalizations for imagenet\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        RandomGreen(64, 64),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((768, 576)),\n",
    "        GetPatches(768, 576, 12, 9),\n",
    "        TransformPatches([\n",
    "            transforms.Resize(224),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, global_step, loss, fname):\n",
    "    \"\"\"Save model & optimizer state together with epoch, global step and running loss to fname.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, fname)\n",
    "\n",
    "\n",
    "def log_training(running_loss, training_acc, logging_steps, global_step, learning_rate, scheduler, lr_policy, tb_writer):\n",
    "    \"\"\"Logs training error and f1 using tensorboard\"\"\"\n",
    "    logging_loss, logging_acc = running_loss / logging_steps, training_acc / logging_steps\n",
    "    print('Global step %5d running train loss: %.3f, running train acc: %.3f' %\n",
    "          (global_step, logging_loss, training_acc))\n",
    "    tb_writer.add_scalar(\"loss/train\", logging_loss, global_step)\n",
    "    tb_writer.add_scalar(\"acc/train\", training_acc, global_step)\n",
    "    if lr_policy == \"onecycle\":\n",
    "        tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    vote_count = {l: 0 for l in range(5)}\n",
    "    for l in labels:\n",
    "        vote_count[l] += 1\n",
    "    return max(vote_count, key = vote_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, global_step, training_start_time, tb_writer):\n",
    "    \"\"\"Runs model on validation set and writes results using tensorboard\"\"\"\n",
    "    model.eval()\n",
    "    logits_all = torch.zeros((len(val_dset), 5), dtype=float)\n",
    "    labels_all = torch.zeros(len(val_dset), dtype=int)\n",
    "    preds_all = torch.zeros(len(val_dset), dtype=int)\n",
    "    i = 0\n",
    "    for patches_list, labels in tqdm(val_dataloader):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        bs = len(labels)\n",
    "        labels = labels.to(device)\n",
    "        patches_logits = torch.zeros((bs, len(patches_list), 5), dtype=float)\n",
    "        for patches_idx, patches_batch in enumerate(patches_list):\n",
    "            patches_batch = patches_batch.float().to(device)\n",
    "            patches_logits[:, patches_idx, :] = model.forward(patches_batch)\n",
    "        import pdb; pdb.set_trace()    \n",
    "        patches_losses = F.cross_entropy(patch_logits, labels)\n",
    "        patch_preds = patch_logits.cpu().numpy().argmax(axis=-1)\n",
    "    \n",
    "#         preds_all[]\n",
    "        \n",
    "            \n",
    "        logits_all[i:(i+bs), :] = model.forward(imgs)\n",
    "        labels_all[i:(i+bs)] = labels\n",
    "        i += bs\n",
    "\n",
    "    val_loss = F.cross_entropy(logits_all, labels_all)\n",
    "    preds_all = logits_all.cpu().numpy().argmax(axis=-1)\n",
    "    val_acc = np.sum(labels_all.cpu().numpy() == preds_all) / i\n",
    "\n",
    "    print(\"Time to global step %d took %.1f sec, val loss %.3f, val acc %.3f\" % (\n",
    "        global_step, time.time() - training_start_time, val_loss, val_acc))\n",
    "\n",
    "    tb_writer.add_scalar(\"loss/val\", val_loss, global_step)\n",
    "    tb_writer.add_scalar(\"acc/val\", val_acc, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "validate_model(model, 0, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote([1, 2, 2, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"/mnt/hdd/leaf-disease-outputs\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "logging_dir = Path(\"./runs\")\n",
    "logging_dir.mkdir(exist_ok=True)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 1e-6\n",
    "final_layers_lr = 1e-4\n",
    "weight_decay = 0.0\n",
    "final_layers_wd = 0.0\n",
    "efficientnet_args ={\n",
    "    \"model_name\": 'efficientnet-b7',\n",
    "    \"num_classes\": 5\n",
    "}\n",
    "\n",
    "\n",
    "logging_steps = 500\n",
    "save_checkpoints = True\n",
    "\n",
    "train_dset = LeafDataset(\"./data/train_images\", \"./data/train_images/labels.csv\", transform=data_transforms[\"train\"])\n",
    "val_dset = LeafDataset(\"./data/val_images\", \"./data/val_images/labels.csv\", transform=data_transforms[\"val\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = EfficientNet.from_pretrained(**efficientnet_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for different learning rates/regularization strenghts for final layers\n",
    "final_layers = ['_fc.weight',\n",
    "                '_fc.bias']\n",
    "\n",
    "if final_layers_lr == -1.0:\n",
    "    final_layers_lr = learning_rate\n",
    "if final_layers_wd == -1.0:\n",
    "    final_layers_wd = weight_decay\n",
    "\n",
    "final_layer_params = [(n, p) for n, p in model.named_parameters() if n in final_layers]\n",
    "non_final_layer_params = [(n, p) for n, p in model.named_parameters() if n not in final_layers]\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "final_layer_decaying_params = [p for n, p in final_layer_params if not any(nd in n for nd in no_decay)]\n",
    "final_layer_nondecaying_params = [p for n, p in final_layer_params if any(nd in n for nd in no_decay)]\n",
    "\n",
    "non_final_layer_decaying_params = [p for n, p in non_final_layer_params if not any(nd in n for nd in no_decay)]\n",
    "non_final_layer_nondecaying_params = [p for n, p in non_final_layer_params if any(nd in n for nd in no_decay)]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': final_layer_decaying_params,\n",
    "            'lr':final_layers_lr,\n",
    "            'weight_decay':final_layers_wd},\n",
    "        {'params': final_layer_nondecaying_params,\n",
    "            'lr':final_layers_lr,\n",
    "            'weight_decay':0.0},\n",
    "        {'params': non_final_layer_decaying_params,\n",
    "            'lr':learning_rate,\n",
    "            'weight_decay':weight_decay},\n",
    "        {'params': non_final_layer_nondecaying_params,\n",
    "            'lr':learning_rate,\n",
    "            'weight_decay':0.0},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "lr_policy = \"onecycle\"\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=math.ceil(len(train_dset) / batch_size), epochs=n_epochs)\n",
    "\n",
    "# Output and loggind directories\n",
    "model_prefix = f\"{datetime.now().strftime('%b%d_%H-%M-%S')}\"\n",
    "output_dir = save_dir / model_prefix\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "(output_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "hyperparameters_dict = {\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"l2_weight_decay\": weight_decay,\n",
    "    \"efficientnet_args\": efficientnet_args\n",
    "}\n",
    "with open(output_dir / \"hyperparamters_dict.json\", \"w\") as f:\n",
    "    json.dump(hyperparameters_dict, f)\n",
    "\n",
    "script_path = Path(os.getcwd()) / \"patches.ipynb\"\n",
    "shutil.copy(script_path, output_dir / script_path.name)\n",
    "\n",
    "git_cmd_result = subprocess.run(['git', 'rev-parse', 'HEAD'], stdout=subprocess.PIPE)\n",
    "\n",
    "with open(output_dir / \"commit.txt\", \"w\") as f:\n",
    "    f.write(git_cmd_result.stdout.decode(\"utf-8\"))\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=logging_dir / model_prefix)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_i = 0\n",
    "running_preds = np.zeros(logging_steps * batch_size, dtype=int)\n",
    "running_labels = np.zeros(logging_steps * batch_size, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model = model.to(device)\n",
    "global_step = 1\n",
    "for epoch in range(n_epochs):\n",
    "    tic = time.time()\n",
    "    for imgs, labels in tqdm(train_dataloader):\n",
    "        model.train()\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model.forward(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Metrics and logging\n",
    "        running_loss += loss.mean().item()\n",
    "        with torch.no_grad():\n",
    "            running_preds[running_i:(running_i + logits.shape[0])] = logits.detach().cpu().numpy().argmax(axis=-1)\n",
    "            running_labels[running_i:(running_i + logits.shape[0])] = labels.detach().cpu().numpy()\n",
    "            running_i += logits.shape[0]\n",
    "            if global_step % logging_steps == 0:\n",
    "                # Record loss & acc on training set (over last logging_steps) and validation set\n",
    "                training_acc = np.sum(running_preds == running_labels) / running_i\n",
    "                log_training(running_loss, training_acc, logging_steps, global_step, learning_rate, scheduler, lr_policy,\n",
    "                             tb_writer)\n",
    "                running_loss, running_i = 0.0, 0\n",
    "                running_preds.fill(0)\n",
    "                running_labels.fill(0)\n",
    "\n",
    "                validate_model(model, global_step, tic, tb_writer)\n",
    "\n",
    "                # Save model\n",
    "                if save_checkpoints:\n",
    "                    save_model(model, optimizer, epoch, global_step, running_loss,\n",
    "                               output_dir / \"checkpoints\" / f\"{global_step}.pt\")\n",
    "        global_step += 1\n",
    "\n",
    "\n",
    "    # Validation at end of each epoch\n",
    "    with torch.no_grad():\n",
    "        validate_model(model, global_step, tic, tb_writer)\n",
    "    # Save model\n",
    "    if save_checkpoints:\n",
    "        save_model(model, optimizer, epoch, global_step, running_loss, output_dir / \"checkpoints\" / f\"{global_step}.pt\")\n",
    "\n",
    "tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
